{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.12.2 64-bit ('venv2')",
   "metadata": {
    "interpreter": {
     "hash": "03487c50d935b3e404dd753985c7279e44fb229f54955b1da4b4970915666a9f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scenario\n",
    "Suppose:\n",
    "A classifier that predicts 3 animal classes:\n",
    "\n",
    "üê± 0 = Cat\n",
    "\n",
    "üê∂ 1 = Dog\n",
    "\n",
    "üê∞ 2 = Rabbit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Classes: 0 = Cat, 1 = Dog, 2 = Rabbit\n",
    "y_true = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 1, 1, 1, 2, 2, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro average: unweighted mean\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average: takes support (number of true instances) into account\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Macro Precision: 0.67\nMacro Recall: 0.67\nMacro F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro Precision: {precision_macro:.2f}\")\n",
    "print(f\"Macro Recall: {recall_macro:.2f}\")\n",
    "print(f\"Macro F1 Score: {f1_macro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weighted Precision: 0.67\nWeighted Recall: 0.67\nWeighted F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weighted Precision: {precision_weighted:.2f}\")\n",
    "print(f\"Weighted Recall: {recall_weighted:.2f}\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nClassification Report:\n\n              precision    recall  f1-score   support\n\n         Cat       0.67      0.67      0.67         3\n         Dog       0.67      0.67      0.67         3\n      Rabbit       0.67      0.67      0.67         3\n\n    accuracy                           0.67         9\n   macro avg       0.67      0.67      0.67         9\nweighted avg       0.67      0.67      0.67         9\n\n"
     ]
    }
   ],
   "source": [
    "# Detailed breakdown for each class\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Cat\", \"Dog\", \"Rabbit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Formulas\n",
    "In multi-class, you treat one class at a time as the ‚Äúpositive‚Äù class, and all others as ‚Äúnegative.‚Äù\n",
    "\n",
    "So for class i:\n",
    "\n",
    "True Positives (TP‚Çñ): Number of samples correctly predicted as class k.\n",
    "\n",
    "False Positives (FP‚Çñ): Number of samples predicted as class k but actually belong to another class.\n",
    "\n",
    "False Negatives (FN‚Çñ): Number of samples that actually belong to class k but were predicted as another class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision for class k\n",
    "\n",
    "Precisionk=TPk/TPk+FPk\n",
    "‚Äã\n",
    "Recall for class k\n",
    "\n",
    "Recallk=TPk/TPk+FNk\n",
    "‚Äã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 Score for class k\n",
    "F1k=2√óPrecisionk√óRecallk/Precisionk+Recallk\n",
    "‚Äã\n",
    " \n",
    "‚Äã\n",
    " \n"
   ]
  }
 ]
}